"""
å¢å¼ºç‰ˆ MTUCB + Ollama LLM é›†æˆç³»ç»Ÿ
å…¨é¢ä¼˜åŒ–SC-QOSï¼ˆè¯­ä¹‰é€šä¿¡æœåŠ¡è´¨é‡ï¼‰ï¼Œæ”¯æŒ6ä¸ªå‚æ•°åŠ¨æ€è°ƒæ•´

æ–°å¢ç‰¹æ€§ï¼š
1. 6å‚æ•°å…¨é¢ä¼˜åŒ–ï¼šalpha, zeta, omega, compression_ratio, power_ratio, min_phi
2. è¯­ä¹‰é€šä¿¡å¢å¼ºï¼šè¯­ä¹‰å‹ç¼©ã€åŠŸç‡åˆ†é…ã€å¤šçº§QOS
3. æ™ºèƒ½ç½‘ç»œçŠ¶æ€æ„ŸçŸ¥ï¼šæ‹¥å¡é¢„æµ‹ã€è´Ÿè½½å‡è¡¡ã€åˆ‡æ¢ä¼˜åŒ–
4. ä¸°å¯Œçš„æ€§èƒ½æŒ‡æ ‡ï¼šè¯­ä¹‰æ•ˆç‡ã€èƒ½è€—æ•ˆç‡ã€ç”¨æˆ·ä½“éªŒè´¨é‡
5. å¤šç»´åº¦å¯è§†åŒ–ï¼šçƒ­åŠ›å›¾ã€é›·è¾¾å›¾ã€è¶‹åŠ¿åˆ†æ
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
from typing import List, Tuple, Dict, Optional
import time
from tqdm import tqdm
from dataclasses import dataclass, asdict
import pandas as pd

# å¯¼å…¥åŸæœ‰æ¨¡å—
from ollama_integration import OllamaLLM
from llm_suggest import NetworkMetrics, LLMSuggestion
from sc_qos_optimizer import SCQoSOptimizer, SCQoSConfig
from llm_qos_evaluator import LLMQoSEvaluator

# è®¾ç½®éšæœºç§å­
np.random.seed(42)
random.seed(42)

# è®¾ç½®matplotlibæ”¯æŒä¸­æ–‡
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100


@dataclass
class EnhancedNetworkMetrics:
    """å¢å¼ºçš„ç½‘ç»œæ€§èƒ½æŒ‡æ ‡"""
    # åŸºç¡€QoSæŒ‡æ ‡
    avg_qos: float
    avg_effective_qos: float
    avg_objective_score: float
    semantic_accuracy: float
    semantic_rate: float
    llm_semantic_score: float
    user_satisfaction: float

    # æ—¶å»¶ä¸èƒ½è€—
    avg_latency_ms: float
    avg_energy_joule: float

    # ç½‘ç»œçŠ¶æ€æŒ‡æ ‡
    path_congestion: List[float]
    worker_load: List[float]
    switching_rate: float

    # è¯­ä¹‰é€šä¿¡æŒ‡æ ‡
    semantic_compression_efficiency: float  # è¯­ä¹‰å‹ç¼©æ•ˆç‡
    power_efficiency: float                 # åŠŸç‡æ•ˆç‡
    latency_variance: float                 # å»¶è¿Ÿæ–¹å·®
    bandwidth_utilization: float            # å¸¦å®½åˆ©ç”¨ç‡

    # ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
    service_continuity: float               # æœåŠ¡è¿ç»­æ€§
    qoe_score: float                       # ç”¨æˆ·ä½“éªŒè´¨é‡
    fairness_index: float                  # å…¬å¹³æ€§æŒ‡æ ‡

    # ç³»ç»Ÿæ•ˆç‡æŒ‡æ ‡
    resource_efficiency: float             # èµ„æºæ•ˆç‡
    energy_consumption: float              # å½’ä¸€åŒ–èƒ½è€—æŒ‡æ ‡ï¼ˆ0-1ï¼‰
    system_stability: float                # ç³»ç»Ÿç¨³å®šæ€§

    timestamp: int


class EnhancedMTUCBBaseline:
    """å¢å¼ºçš„åŸºç¡€MTUCBç®—æ³•ï¼ˆå›ºå®šå‚æ•°ï¼‰"""
    
    def __init__(self, num_users=12, num_workers=6, num_paths=4):
        self.num_users = num_users
        self.num_workers = num_workers
        self.num_paths = num_paths
        
        # SC-QOSå›ºå®šå‚æ•°é…ç½®
        self.config = SCQoSConfig.default()
        
        # ç¯å¢ƒå‚æ•°
        self.beta = 0.8  # å·¥äººåå¥½æƒé‡

        # âœ… å¼‚æ„å·¥äººå»ºæ¨¡
        self.worker_capacity_profile = np.random.randint(1, 4, size=self.num_workers)
        self.worker_capacity = int(np.round(np.mean(self.worker_capacity_profile)))  # å‘åå…¼å®¹
        self.timeslot_duration_ms = 120.0  # å•ä¸ªæ—¶éš™æŒç»­æ—¶é—´ï¼Œç”¨äºèƒ½è€—ä¼°è®¡
        self.reference_latency_ms = 250.0
        self.objective_weights = {
            'qos': 0.6,
            'delay': 0.25,
            'energy': 0.15
        }
        
        # åˆå§‹åŒ–ç¯å¢ƒ
        self._init_enhanced_environment()

        # LLM semantic QoS evaluator
        self.llm_qos_evaluator = LLMQoSEvaluator(auto_generate=True)
        # Weighting between legacy QoS and LLM semantic QoS contribution
        self.qos_weight = 0.7
        self.semantic_weight = 0.3
        self.llm_quality_factor = 1.0  # åŠ¨æ€è´¨é‡å› å­ï¼Œå¤–éƒ¨æ–¹æ³•å¯è®¾ç½®
        
        # æ€§èƒ½è®°å½•
        self.metrics_history: List[EnhancedNetworkMetrics] = []
        self.qos_history = []
        self.effective_qos_history = []
        self.objective_score_history = []
        self.latency_history_ms = []
        self.energy_history_joule = []
        self.semantic_accuracy_history = []
        self.semantic_rate_history = []
        self.llm_semantic_score_history = []
        
        # æ–°å¢æ€§èƒ½è®°å½•
        self.compression_efficiency_history = []
        self.power_efficiency_history = []
        self.qoe_history = []
        self.resource_efficiency_history = []
        self.energy_consumption_history = []
        
        # åŒ¹é…å’Œé€‰æ‹©è®°å½•
        self.R = np.zeros((num_users, num_workers, num_paths))
        self.S = np.zeros((num_users, num_workers, num_paths))
        self.historical_matches = {u: [] for u in range(num_users)}
        
        # ç½‘ç»œçŠ¶æ€è®°å½•
        self.congestion_history = []
        self.load_balance_history = []
        self.switching_history = []
        
        # å‚æ•°å†å²ï¼ˆåŸºç¡€ç‰ˆä¹Ÿè®°å½•å›ºå®šå‚æ•°ï¼Œä¾¿äºç»Ÿä¸€å¯è§†åŒ–ï¼‰
        self.parameter_history = {
            "alpha": [], "zeta": [], "omega": [],
            "compression_ratio": [], "power_ratio": [], "min_phi": []
        }

    def _init_enhanced_environment(self):
        """åˆå§‹åŒ–å¢å¼ºçš„ä»¿çœŸç¯å¢ƒ"""
        # åŸºç¡€å…¼å®¹æ€§çŸ©é˜µ
        self.compatibility_matrix = np.random.uniform(0.5, 1.0, 
                                                    (self.num_users, self.num_workers))
        
        # ç”¨æˆ·æ”¯ä»˜æ„æ„¿å’ŒæœåŠ¡éœ€æ±‚
        self.willingness_to_pay = np.random.uniform(0.3, 1.0, self.num_users)
        self.service_priority = np.random.uniform(0.4, 1.0, self.num_users)  # æœåŠ¡ä¼˜å…ˆçº§
        
        # è·¯å¾„åŸºç¡€è´¨é‡å’Œç‰¹å¾
        self.base_path_qualities = np.random.uniform(0.4, 0.9, 
                                                   (self.num_workers, self.num_paths))
        self.path_latency = np.random.uniform(10, 100, (self.num_workers, self.num_paths))  # ms
        self.path_bandwidth = np.random.uniform(50, 200, (self.num_workers, self.num_paths))  # Mbps
        
        # è¯­ä¹‰é€šä¿¡ç‰¹å¾
        self.semantic_complexity = np.random.uniform(0.3, 0.8, self.num_users)  # è¯­ä¹‰å¤æ‚åº¦
        self.compression_capability = np.random.uniform(0.5, 0.9, self.num_workers)  # å‹ç¼©èƒ½åŠ›
        
        # èƒ½è€—æ¨¡å‹
        self.base_power_consumption = np.random.uniform(50, 150, self.num_workers)  # W
        self.power_scaling_factor = np.random.uniform(0.8, 1.2, self.num_workers)

        # å¼‚æ„æ€§å‚æ•°
        self.worker_delay_bias = np.random.uniform(0.8, 1.3, self.num_workers)
        self.worker_energy_factor = np.random.uniform(0.9, 1.4, self.num_workers)
        self.worker_latency_jitter = np.random.uniform(5.0, 20.0, self.num_workers)  # ms

        # å‚è€ƒèƒ½è€—ä¸Šé™ï¼ˆç”¨äºå½’ä¸€åŒ–ï¼‰
        reference_power = np.max(self.base_power_consumption * self.power_scaling_factor * self.worker_energy_factor)
        self.reference_energy = reference_power * (self.timeslot_duration_ms / 1000.0)
    
    def get_enhanced_path_quality(self, t: int, worker: int, path: int) -> Dict[str, float]:
        """è·å–å¢å¼ºçš„è·¯å¾„è´¨é‡ä¿¡æ¯"""
        base_quality = self.base_path_qualities[worker, path]
        
        # æ—¶é—´å˜åŒ–çš„ç½‘ç»œçŠ¶æ€
        if 50 <= t <= 100:  # æ‹¥å¡æœŸ
            congestion_factor = 0.6 + 0.3 * np.sin(0.3 * (t - 50))
            latency_penalty = 1.5
        elif 100 < t <= 150:  # é¢‘ç¹åˆ‡æ¢æœŸ
            switch_factor = 0.7 + 0.4 * np.sin(0.5 * (t - 100)) * np.random.normal(1, 0.2)
            congestion_factor = switch_factor
            latency_penalty = 1.2
        else:
            congestion_factor = 1.0
            latency_penalty = 1.0
        
        # è®¡ç®—å„é¡¹è´¨é‡æŒ‡æ ‡
        quality = base_quality * congestion_factor + 0.05 * np.sin(0.1 * t + worker + path) * np.random.normal(0, 0.1)
        quality = np.clip(quality, 0.1, 1.0)
        
        # å»¶è¿Ÿå’Œå¸¦å®½ï¼ˆè€ƒè™‘å·¥äººå¼‚æ„æ€§ï¼‰
        base_latency = self.path_latency[worker, path] * latency_penalty * self.worker_delay_bias[worker]
        latency_jitter = np.random.normal(0, self.worker_latency_jitter[worker])
        current_latency = max(5.0, base_latency + latency_jitter)
        current_bandwidth = self.path_bandwidth[worker, path] / latency_penalty
        
        return {
            'quality': quality,
            'latency': current_latency,
            'bandwidth': current_bandwidth,
            'congestion_level': 1 - congestion_factor
        }
    
    def calculate_semantic_compression_efficiency(self, user: int, worker: int, path: int) -> float:
        """è®¡ç®—è¯­ä¹‰å‹ç¼©æ•ˆç‡"""
        user_complexity = self.semantic_complexity[user]
        worker_capability = self.compression_capability[worker]
        compression_ratio = self.config.compression_ratio
        
        # è€ƒè™‘ç”¨æˆ·è¯­ä¹‰å¤æ‚åº¦å’Œå·¥äººå‹ç¼©èƒ½åŠ›
        efficiency = worker_capability * (1 - user_complexity * 0.3) * compression_ratio
        return np.clip(efficiency, 0.1, 1.0)
    
    def calculate_power_efficiency(self, worker: int, load_ratio: float) -> Tuple[float, float]:
        """è®¡ç®—åŠŸç‡æ•ˆç‡å’Œå®é™…åŠŸç‡"""
        base_power = self.base_power_consumption[worker]
        scaling = self.power_scaling_factor[worker] * self.worker_energy_factor[worker]
        power_ratio = self.config.power_ratio

        # åŠŸç‡éšè´Ÿè½½å¢åŠ ï¼ˆä½¿ç”¨å½’ä¸€åŒ–è´Ÿè½½ï¼‰
        actual_power = base_power * (1 + 0.5 * load_ratio) * scaling * power_ratio

        # æ•ˆç‡è®¡ç®—ï¼ˆæœåŠ¡è´¨é‡/åŠŸè€—ï¼‰
        efficiency = 1.0 / (1 + actual_power / 120)  # å½’ä¸€åŒ–ï¼Œå‚è€ƒåŠŸç‡120W
        efficiency = np.clip(efficiency, 0.1, 1.0)
        return efficiency, actual_power
    
    def calculate_enhanced_qos(self, t: int, user: int, worker: int, path: int,
                             worker_load: int) -> Dict[str, float]:
        """?????QoS??????????????LLM?????"""
        path_info = self.get_enhanced_path_quality(t, worker, path)
        compatibility = self.compatibility_matrix[user, worker]

        base_qos = (
            self.config.alpha * path_info['quality']
            + (1 - self.config.alpha) * compatibility
        )

        # ????????????????????????????
        capacity = max(1, self.worker_capacity_profile[worker])
        raw_ratio = worker_load / capacity
        load_ratio = float(np.clip(raw_ratio, 0.2, 4.0))

        if load_ratio <= 1.0:
            relief = 1.0 - load_ratio
            load_factor = 1.0 + 0.15 * relief
        else:
            overload = load_ratio - 1.0
            load_factor = np.exp(-0.45 * overload)
        base_qos *= load_factor

        compression_eff = self.calculate_semantic_compression_efficiency(user, worker, path)
        power_eff, actual_power = self.calculate_power_efficiency(worker, load_ratio)

        semantic_rate = base_qos * compression_eff
        semantic_penalty = 0.8 if semantic_rate < self.config.min_phi else 1.0

        enhanced_qos = base_qos * (1 + 0.2 * compression_eff) * (1 + 0.1 * power_eff) * semantic_penalty
        enhanced_qos = np.clip(enhanced_qos, 0.1, 1.0)

        latency_scale = 1.0 + 0.7 * max(0.0, load_ratio - 1.0)
        latency_ms = path_info['latency'] * latency_scale
        energy_joule = actual_power * (self.timeslot_duration_ms / 1000.0)

        semantic_score = self.llm_qos_evaluator.get_semantic_score(
            service_priority=float(self.service_priority[user]),
            semantic_complexity=float(self.semantic_complexity[user]),
            path_quality=float(path_info['quality']),
            load_ratio=load_ratio,
            bandwidth_mbps=float(path_info.get('bandwidth', 0.0)),
            latency_ms=float(latency_ms),
        )
        combined_qos = self.qos_weight * enhanced_qos + self.semantic_weight * semantic_score

        effective_qos = combined_qos * np.exp(-latency_ms / max(1.0, self.reference_latency_ms))

        norm_delay = latency_ms / self.reference_latency_ms
        norm_energy = energy_joule / (self.reference_energy + 1e-6)
        objective_score = (
            self.objective_weights['qos'] * combined_qos
            - self.objective_weights['delay'] * norm_delay
            - self.objective_weights['energy'] * norm_energy
        )

        semantic_accuracy = np.clip(0.6 * enhanced_qos + 0.4 * semantic_score, 0.1, 1.0)
        if 50 <= t <= 100:
            semantic_accuracy -= 0.1 * (1 - np.cos(0.1 * (t - 50)))
        semantic_accuracy = np.clip(semantic_accuracy * 0.95, 0.1, 1.0)

        return {
            'qos': enhanced_qos,
            'effective_qos': effective_qos,
            'objective_score': objective_score,
            'semantic_accuracy': semantic_accuracy,
            'semantic_rate': semantic_rate,
            'llm_semantic_score': semantic_score,
            'combined_qos': combined_qos,
            'compression_efficiency': compression_eff,
            'power_efficiency': power_eff,
            'power_watt': actual_power,
            'energy_joule': energy_joule,
            'load_ratio': load_ratio,
            'latency': latency_ms,
            'bandwidth': path_info['bandwidth']
        }

    def collect_enhanced_metrics(self, t: int, matching: List[Tuple[int, int, int]], 
                               qos_results: List[Dict[str, float]]) -> EnhancedNetworkMetrics:
        """æ”¶é›†å¢å¼ºçš„ç½‘ç»œæŒ‡æ ‡"""
        if not matching or not qos_results:
            return self._get_default_metrics(t)
        
        # åŸºç¡€æŒ‡æ ‡
        avg_qos = np.mean([r['qos'] for r in qos_results])
        avg_effective_qos = np.mean([r['effective_qos'] for r in qos_results])
        avg_objective_score = np.mean([r['objective_score'] for r in qos_results])
        avg_semantic_accuracy = np.mean([r['semantic_accuracy'] for r in qos_results])
        avg_semantic_rate = np.mean([r['semantic_rate'] for r in qos_results])
        avg_semantic_score = np.mean([r.get('llm_semantic_score', 0.0) for r in qos_results])
        avg_latency_ms = np.mean([r['latency'] for r in qos_results])
        avg_energy_joule = np.mean([r['energy_joule'] for r in qos_results])
        
        # è¯­ä¹‰é€šä¿¡æŒ‡æ ‡
        compression_efficiency = np.mean([r['compression_efficiency'] for r in qos_results])
        power_efficiency = np.mean([r['power_efficiency'] for r in qos_results])
        
        # ç½‘ç»œæ€§èƒ½æŒ‡æ ‡
        latency_variance = np.var([r['latency'] for r in qos_results])
        bandwidth_utilization = np.mean([r['bandwidth'] for r in qos_results]) / 200  # å½’ä¸€åŒ–
        
        # è·¯å¾„æ‹¥å¡å’Œå·¥äººè´Ÿè½½
        path_congestion = []
        worker_load = []
        for w in range(self.num_workers):
            worker_matches = sum(1 for _, worker, _ in matching if worker == w)
            capacity = max(1, self.worker_capacity_profile[w])
            congestion = worker_matches / capacity
            path_congestion.append(congestion)
            worker_load.append(congestion)
        
        # åˆ‡æ¢ç‡
        switches = 0
        total_users = 0
        for u in range(self.num_users):
            if len(self.historical_matches[u]) >= 2:
                if self.historical_matches[u][-1][1] != self.historical_matches[u][-2][1]:
                    switches += 1
                total_users += 1
        switching_rate = switches / total_users if total_users > 0 else 0
        
        # ç”¨æˆ·æ»¡æ„åº¦å’Œä½“éªŒè´¨é‡
        user_satisfaction = avg_qos * 0.85 + np.random.normal(0, 0.02)
        user_satisfaction = np.clip(user_satisfaction, 0, 1)
        
        # QoEè¯„åˆ†ï¼ˆè€ƒè™‘å¤šä¸ªå› ç´ ï¼‰
        qoe_score = (
            0.32 * avg_qos
            + 0.22 * avg_semantic_accuracy
            + 0.16 * avg_semantic_score
            + 0.18 * compression_efficiency
            + 0.12 * power_efficiency
        )
        qoe_score = np.clip(qoe_score, 0, 1)
        
        # æœåŠ¡è¿ç»­æ€§ï¼ˆåŸºäºåˆ‡æ¢é¢‘ç‡ï¼‰
        service_continuity = max(0, 1 - switching_rate * 2)
        
        # å…¬å¹³æ€§æŒ‡æ ‡ï¼ˆJain's fairness indexï¼‰
        qos_values = [r['qos'] for r in qos_results]
        fairness_index = (sum(qos_values) ** 2) / (len(qos_values) * sum(q ** 2 for q in qos_values))
        
        # èµ„æºæ•ˆç‡
        resource_efficiency = avg_qos / max(np.mean(worker_load), 0.1)
        resource_efficiency = np.clip(resource_efficiency, 0, 2)
        
        # èƒ½è€—ï¼ˆå½’ä¸€åŒ–ï¼‰
        energy_norms = [r['energy_joule'] / (self.reference_energy + 1e-6) for r in qos_results]
        energy_consumption = np.clip(np.mean(energy_norms), 0, 1)
        
        # ç³»ç»Ÿç¨³å®šæ€§
        qos_std = np.std(qos_values)
        system_stability = max(0, 1 - qos_std * 2)
        
        return EnhancedNetworkMetrics(
            avg_qos=avg_qos,
            avg_effective_qos=avg_effective_qos,
            avg_objective_score=avg_objective_score,
            semantic_accuracy=avg_semantic_accuracy,
            semantic_rate=avg_semantic_rate,
            llm_semantic_score=avg_semantic_score,
            user_satisfaction=user_satisfaction,
            avg_latency_ms=avg_latency_ms,
            avg_energy_joule=avg_energy_joule,
            path_congestion=path_congestion,
            worker_load=worker_load,
            switching_rate=switching_rate,
            semantic_compression_efficiency=compression_efficiency,
            power_efficiency=power_efficiency,
            latency_variance=latency_variance,
            bandwidth_utilization=bandwidth_utilization,
            service_continuity=service_continuity,
            qoe_score=qoe_score,
            fairness_index=fairness_index,
            resource_efficiency=resource_efficiency,
            energy_consumption=energy_consumption,
            system_stability=system_stability,
            timestamp=t
        )

    def compute_optimal_qos_for_timestep(self, t: int) -> float:
        """
        è®¡ç®—æ—¶éš™ t çš„è´ªå¿ƒæœ€ä¼˜ QoS ä¸Šç•Œï¼ˆç”¨äºé—æ†¾å€¼åŸºå‡†ï¼‰
        å‚è€ƒ MTUCB._calculate_optimal_qos çš„æ ¸å¿ƒæ€è·¯ï¼šå®¹é‡çº¦æŸä¸‹ä¸ºæ¯ä¸ªç”¨æˆ·æŒ‘é€‰æœ€ä¼˜å·¥äºº-è·¯å¾„ç»„åˆ
        """
        best_total_qos = 0.0
        worker_loads = {w: 0 for w in range(self.num_workers)}

        for u in range(self.num_users):
            best_qos_for_user = 0.0
            best_worker = None

            for w in range(self.num_workers):
                capacity = max(1, self.worker_capacity_profile[w])
                if worker_loads[w] >= capacity:
                    continue

                for p in range(self.num_paths):
                    qos_val = self.calculate_enhanced_qos(t, u, w, p, worker_loads[w])
                    if isinstance(qos_val, dict):
                        qos_scalar = qos_val.get('qos', 0.0)
                    else:
                        qos_scalar = float(qos_val)

                    if qos_scalar > best_qos_for_user:
                        best_qos_for_user = qos_scalar
                        best_worker = w

            if best_worker is not None and best_qos_for_user > 0:
                worker_loads[best_worker] += 1
            best_total_qos += best_qos_for_user

        return float(best_total_qos)

    def compute_optimal_objective_for_timestep(self, t: int) -> float:
        """
        ä½¿ç”¨ä¸å®æ—¶å†³ç­–ä¸€è‡´çš„ç»¼åˆ objective_score è®¡ç®—è´ªå¿ƒæœ€ä¼˜ä¸Šç•Œã€‚

        é€»è¾‘ä¸ compute_optimal_qos_for_timestep ä¸€è‡´ï¼Œä½†åŸºäº objective_score
       ï¼ˆå³ QoS âˆ’ delay_penalty âˆ’ energy_penaltyï¼‰ï¼Œç¡®ä¿ reward/regret å£å¾„ç»Ÿä¸€ã€‚
        """
        best_total_objective = 0.0
        worker_loads = {w: 0 for w in range(self.num_workers)}

        for u in range(self.num_users):
            best_obj_for_user = float('-inf')
            best_worker = None

            for w in range(self.num_workers):
                capacity = max(1, self.worker_capacity_profile[w])
                if worker_loads[w] >= capacity:
                    continue

                for p in range(self.num_paths):
                    qos_val = self.calculate_enhanced_qos(t, u, w, p, worker_loads[w])
                    if isinstance(qos_val, dict):
                        obj_scalar = qos_val.get('objective_score')
                    else:
                        obj_scalar = None

                    if obj_scalar is None:
                        continue

                    if obj_scalar > best_obj_for_user:
                        best_obj_for_user = obj_scalar
                        best_worker = w

            if best_worker is not None and best_obj_for_user > float('-inf'):
                worker_loads[best_worker] += 1
                best_total_objective += best_obj_for_user

        return float(best_total_objective)

    def _get_default_metrics(self, t: int) -> EnhancedNetworkMetrics:
        """è·å–é»˜è®¤æŒ‡æ ‡ï¼ˆå½“æ²¡æœ‰åŒ¹é…æ—¶ï¼‰"""
        return EnhancedNetworkMetrics(
            avg_qos=0.0,
            avg_effective_qos=0.0,
            avg_objective_score=0.0,
            semantic_accuracy=0.0,
            semantic_rate=0.0,
            llm_semantic_score=0.0,
            user_satisfaction=0.0,
            avg_latency_ms=0.0,
            avg_energy_joule=0.0,
            path_congestion=[0.0] * self.num_workers,
            worker_load=[0.0] * self.num_workers,
            switching_rate=0.0,
            semantic_compression_efficiency=0.0,
            power_efficiency=0.0,
            latency_variance=0.0,
            bandwidth_utilization=0.0,
            service_continuity=0.0,
            qoe_score=0.0,
            fairness_index=0.0,
            resource_efficiency=0.0,
            energy_consumption=0.0,
            system_stability=0.0,
            timestamp=t
        )
    
    def calculate_preference(self, t: int, user: int, worker: int) -> float:
        """è®¡ç®—ç”¨æˆ·å¯¹å·¥äººçš„åå¥½"""
        total_reward = sum(self.R[user, worker, p] for p in range(self.num_paths))
        total_selections = sum(self.S[user, worker, p] for p in range(self.num_paths))
        
        if total_selections > 0:
            avg_reward = total_reward / total_selections
        else:
            avg_reward = self.compatibility_matrix[user, worker]
        
        ucb_term = np.sqrt(self.config.zeta * np.log(t + 1) / (total_selections + 1))
        
        last_match = self.historical_matches[user][-1][1] if self.historical_matches[user] else None
        switching_cost = self.config.omega if (last_match is not None and last_match != worker) else 0
        
        return avg_reward + ucb_term - switching_cost
    
    def stable_matching(self, t: int, users: Optional[List[int]] = None) -> List[Tuple[int, int]]:
        """Gale-Shapleyç¨³å®šåŒ¹é…ï¼ˆå¯é€‰ç”¨æˆ·å­é›†ï¼‰"""
        free_users = list(users) if users is not None else list(range(self.num_users))
        matches = {w: [] for w in range(self.num_workers)}
        proposals = {u: set() for u in range(self.num_users)}
        
        while free_users:
            u = free_users.pop(0)
            
            preferences = sorted(range(self.num_workers), 
                               key=lambda w: self.calculate_preference(t, u, w), 
                               reverse=True)
            
            for w in preferences:
                if w not in proposals[u]:
                    proposals[u].add(w)
                    
                    if len(matches[w]) < self.worker_capacity_profile[w]:
                        matches[w].append(u)
                        break
                    else:
                        def worker_preference(user):
                            compatibility = self.compatibility_matrix[user, w]
                            willingness = self.willingness_to_pay[user]
                            priority = self.service_priority[user]
                            return (self.beta * compatibility + 
                                   (1 - self.beta) * willingness * 0.5 + 
                                   priority * 0.3)
                        
                        worst_u = min(matches[w], key=worker_preference)
                        if worker_preference(u) > worker_preference(worst_u):
                            matches[w].remove(worst_u)
                            matches[w].append(u)
                            free_users.append(worst_u)
                            break
        
        matching = []
        for w in range(self.num_workers):
            for u in matches[w]:
                matching.append((u, w))
        
        return matching
    
    def select_path_ucb(self, t: int, user: int, worker: int) -> int:
        """å¢å¼ºçš„UCBè·¯å¾„é€‰æ‹©"""
        best_score = float('-inf')
        best_path = 0
        
        for p in range(self.num_paths):
            if self.S[user, worker, p] > 0:
                avg_reward = self.R[user, worker, p] / self.S[user, worker, p]
            else:
                avg_reward = self.compatibility_matrix[user, worker]
            
            ucb_term = np.sqrt(self.config.zeta * np.log(t + 1) / (self.S[user, worker, p] + 1))
            
            # è€ƒè™‘å½“å‰è·¯å¾„è´¨é‡
            path_info = self.get_enhanced_path_quality(t, worker, p)
            quality_bonus = 0.1 * path_info['quality']
            
            score = avg_reward + ucb_term + quality_bonus
            
            if score > best_score:
                best_score = score
                best_path = p
        
        return best_path
    
    def run_simulation(self, T: int = 200):
        """è¿è¡Œå¢å¼ºä»¿çœŸ"""
        print(f"ğŸš€ è¿è¡Œå¢å¼ºMTUCBä»¿çœŸ (T={T})")
        
        for t in tqdm(range(T), desc="ä»¿çœŸè¿›åº¦"):
            matching = self.stable_matching(t)
            
            matching_with_paths = []
            qos_results = []
            worker_loads = {w: 0 for w in range(self.num_workers)}
            
            for (u, w) in matching:
                worker_loads[w] += 1
                path = self.select_path_ucb(t, u, w)
                qos_result = self.calculate_enhanced_qos(t, u, w, path, worker_loads[w])
                
                matching_with_paths.append((u, w, path))
                qos_results.append(qos_result)
                
                self.R[u, w, path] += qos_result['objective_score']
                self.S[u, w, path] += 1
                
                self.historical_matches[u].append((t, w))
                if len(self.historical_matches[u]) > 10:
                    self.historical_matches[u].pop(0)
            
            # æ”¶é›†å¢å¼ºæŒ‡æ ‡
            current_metrics = self.collect_enhanced_metrics(t, matching_with_paths, qos_results)
            self.metrics_history.append(current_metrics)
            
            # è®°å½•åŸºç¡€å†å²
            self.qos_history.append(current_metrics.avg_qos)
            self.effective_qos_history.append(current_metrics.avg_effective_qos)
            self.objective_score_history.append(current_metrics.avg_objective_score)
            self.latency_history_ms.append(current_metrics.avg_latency_ms)
            self.energy_history_joule.append(current_metrics.avg_energy_joule)
            self.semantic_accuracy_history.append(current_metrics.semantic_accuracy)
            self.semantic_rate_history.append(current_metrics.semantic_rate)
            self.llm_semantic_score_history.append(current_metrics.llm_semantic_score)
            
            # è®°å½•æ–°å¢æŒ‡æ ‡
            self.compression_efficiency_history.append(current_metrics.semantic_compression_efficiency)
            self.power_efficiency_history.append(current_metrics.power_efficiency)
            self.qoe_history.append(current_metrics.qoe_score)
            self.resource_efficiency_history.append(current_metrics.resource_efficiency)
            self.energy_consumption_history.append(current_metrics.energy_consumption)

            # è®°å½•å½“å‰å‚æ•°çŠ¶æ€ï¼ˆæ¯ä¸ªæ—¶é—´æ­¥éƒ½è®°å½•ï¼Œç¡®ä¿å‚æ•°å†å²å®Œæ•´ï¼‰
            self.parameter_history["alpha"].append(self.config.alpha)
            self.parameter_history["zeta"].append(self.config.zeta)
            self.parameter_history["omega"].append(self.config.omega)
            self.parameter_history["compression_ratio"].append(self.config.compression_ratio)
            self.parameter_history["power_ratio"].append(self.config.power_ratio)
            self.parameter_history["min_phi"].append(self.config.min_phi)


class EnhancedMTUCBWithOllama(EnhancedMTUCBBaseline):
    """é›†æˆOllama LLMçš„å¢å¼ºMTUCBç®—æ³•"""
    
    def __init__(self, num_users=12, num_workers=6, num_paths=4, 
                 llm_model="tinyllama", llm_period=25):
        super().__init__(num_users, num_workers, num_paths)
        
        self.llm_period = llm_period
        
        # åˆå§‹åŒ–Ollama LLM
        print(f"ğŸ¤– åˆå§‹åŒ–å¢å¼ºOllama LLM (æ¨¡å‹: {llm_model})")
        self.llm = OllamaLLM(llm_model)
        
        # SC-QOSä¼˜åŒ–å™¨
        self.sc_optimizer = SCQoSOptimizer(self.config)
        
        # æ€§èƒ½è®°å½•
        self.llm_suggestions = []
        self.parameter_history = {
            "alpha": [], "zeta": [], "omega": [],
            "compression_ratio": [], "power_ratio": [], "min_phi": []
        }
        self.llm_call_times = []
        self.confidence_history = []
        
        # ä¼˜åŒ–æ•ˆæœè®°å½•
        self.before_optimization = []  # ä¼˜åŒ–å‰æ€§èƒ½
        self.after_optimization = []   # ä¼˜åŒ–åæ€§èƒ½
    
    def run_simulation(self, T: int = 200):
        """è¿è¡ŒåŒ…å«LLMä¼˜åŒ–çš„å¢å¼ºä»¿çœŸ"""
        print(f"ğŸš€ è¿è¡ŒNetLLMå¢å¼ºMTUCBä»¿çœŸ (T={T})")
        
        for t in tqdm(range(T), desc="ä»¿çœŸè¿›åº¦"):
            # è®°å½•å½“å‰å‚æ•°åˆ°å†å²
            self.parameter_history["alpha"].append(self.config.alpha)
            self.parameter_history["zeta"].append(self.config.zeta)
            self.parameter_history["omega"].append(self.config.omega)
            self.parameter_history["compression_ratio"].append(self.config.compression_ratio)
            self.parameter_history["power_ratio"].append(self.config.power_ratio)
            self.parameter_history["min_phi"].append(self.config.min_phi)
            
            # æ‰§è¡ŒåŒ¹é…å’Œè·¯å¾„é€‰æ‹©
            matching = self.stable_matching(t)
            
            matching_with_paths = []
            qos_results = []
            worker_loads = {w: 0 for w in range(self.num_workers)}
            
            for (u, w) in matching:
                worker_loads[w] += 1
                path = self.select_path_ucb(t, u, w)
                qos_result = self.calculate_enhanced_qos(t, u, w, path, worker_loads[w])
                
                matching_with_paths.append((u, w, path))
                qos_results.append(qos_result)
                
                self.R[u, w, path] += qos_result['objective_score']
                self.S[u, w, path] += 1
                
                self.historical_matches[u].append((t, w))
                if len(self.historical_matches[u]) > 10:
                    self.historical_matches[u].pop(0)
            
            # æ”¶é›†å¢å¼ºæŒ‡æ ‡
            current_metrics = self.collect_enhanced_metrics(t, matching_with_paths, qos_results)
            self.metrics_history.append(current_metrics)
            
            # è®°å½•åŸºç¡€å†å²
            self.qos_history.append(current_metrics.avg_qos)
            self.semantic_accuracy_history.append(current_metrics.semantic_accuracy)
            self.semantic_rate_history.append(current_metrics.semantic_rate)
            
            # è®°å½•æ–°å¢æŒ‡æ ‡
            self.compression_efficiency_history.append(current_metrics.semantic_compression_efficiency)
            self.power_efficiency_history.append(current_metrics.power_efficiency)
            self.qoe_history.append(current_metrics.qoe_score)
            self.resource_efficiency_history.append(current_metrics.resource_efficiency)
            self.energy_consumption_history.append(current_metrics.energy_consumption)
            
            # LLMä¼˜åŒ– - æ ¸å¿ƒæ–°å¢é€»è¾‘
            if t % self.llm_period == 0 and t > 0:
                print(f"\nğŸ¤– æ—¶éš™ {t}: NetLLMå‚æ•°ä¼˜åŒ–...")
                self._perform_llm_optimization(t, current_metrics)
    
    def update_parameters_from_suggestion(self, suggestion: LLMSuggestion, t: int):
        """ä»LLMå»ºè®®æ›´æ–°å‚æ•°"""
        # åº”ç”¨LLMå»ºè®®åˆ°SC-QOSä¼˜åŒ–å™¨
        self.sc_optimizer.apply_llm_suggestion(suggestion, t, confidence_threshold=0.5)
        
        # æ›´æ–°å½“å‰é…ç½®
        self.config = self.sc_optimizer.config
        
        # æ³¨æ„ï¼šå‚æ•°å†å²åœ¨ä»¿çœŸä¸»å¾ªç¯ä¸­ç»Ÿä¸€è®°å½•ï¼Œè¿™é‡Œä¸é‡å¤è®°å½•
        
        # è®°å½•ç½®ä¿¡åº¦
        self.confidence_history.append(suggestion.confidence)
    
    def _build_enhanced_metrics_for_llm(self, t: int, current_metrics: "EnhancedNetworkMetrics"):
        """æ„å»ºç”¨äºLLMçš„å¢å¼ºç½‘ç»œæŒ‡æ ‡"""
        # è½¬æ¢ä¸ºNetworkMetricsæ ¼å¼ï¼ˆå…¼å®¹åŸæœ‰æ¥å£ï¼‰
        from llm_suggest import NetworkMetrics
        
        return NetworkMetrics(
            avg_qos=current_metrics.avg_qos,
            semantic_accuracy=current_metrics.semantic_accuracy,
            semantic_rate=current_metrics.semantic_rate,
            user_satisfaction=current_metrics.user_satisfaction,
            path_congestion=current_metrics.path_congestion,
            worker_load=current_metrics.worker_load,
            switching_rate=current_metrics.switching_rate,
            timestamp=t
        )
    
    def _perform_llm_optimization(self, t: int, current_metrics: "EnhancedNetworkMetrics"):
        """æ‰§è¡ŒLLMä¼˜åŒ–"""
        start_time = time.time()
        try:
            # æ„å»ºå¢å¼ºçš„ç½‘ç»œçŠ¶æ€ä¿¡æ¯
            enhanced_metrics = self._build_enhanced_metrics_for_llm(t, current_metrics)

            # åŒºåˆ†NetLLMAdapterå’Œå…¶ä»–LLMæ¥å£
            if hasattr(self.llm, '_build_deep_prompt'):  # æ£€æŸ¥æ˜¯å¦æ˜¯NetLLMAdapter
                suggestion = self.llm.get_suggestion(
                    self.metrics_history[-10:],
                    enhanced_metrics,
                    self.config  # ä¼ é€’ç³»ç»Ÿå½“å‰é…ç½®
                )
            else:
                suggestion = self.llm.get_suggestion(
                    self.metrics_history[-10:],
                    enhanced_metrics
                )

            # è®°å½•ä¼˜åŒ–å‰å‚æ•°
            old_config = asdict(self.config)

            # åº”ç”¨å»ºè®®
            self.sc_optimizer.apply_llm_suggestion(suggestion, t, confidence_threshold=0.5)
            self.config = self.sc_optimizer.config

            # è®°å½•ä¼˜åŒ–åå‚æ•°
            new_config = asdict(self.config)

            elapsed = time.time() - start_time
            self.llm_call_times.append(elapsed)
            self.llm_suggestions.append((t, suggestion))
            self.confidence_history.append(suggestion.confidence)

            print(f"   ğŸ”§ å‚æ•°ä¼˜åŒ–å®Œæˆ (æ¨¡å‹: {self.llm.model_name}):")
            print(f"   Î±: {old_config['alpha']:.3f}â†’{new_config['alpha']:.3f}")
            print(f"   Î¶: {old_config['zeta']:.3f}â†’{new_config['zeta']:.3f}")
            print(f"   Ï‰: {old_config['omega']:.3f}â†’{new_config['omega']:.3f}")
            print(f"   ç½®ä¿¡åº¦: {suggestion.confidence:.3f}")
            print(f"   æ¨ç†: {suggestion.reasoning}")
        except Exception as e:
            print(f"   âŒ LLMä¼˜åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def create_comprehensive_comparison_plots(baseline, llm_enhanced):
    """åˆ›å»ºå…¨é¢çš„æ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
    fig = plt.figure(figsize=(20, 16))
    
    # åˆ›å»ºç½‘æ ¼å¸ƒå±€
    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)
    
    time_steps = range(len(baseline.qos_history))
    
    # 1. ä¸»è¦æ€§èƒ½å¯¹æ¯” (å·¦ä¸Šè§’ï¼Œ2x2)
    ax1 = fig.add_subplot(gs[0:2, 0:2])
    ax1.plot(time_steps, baseline.qos_history, 'b-', linewidth=2.5, 
             label='å›ºå®šå‚æ•°MTUCB', alpha=0.8)
    ax1.plot(time_steps, llm_enhanced.qos_history, 'darkorange', linewidth=2.5, 
             label='åŠ¨æ€å‚æ•°MTUCB+Ollama', alpha=0.9)
    
    # æ ‡è®°å…³é”®æ—¶æœŸ
    ax1.axvspan(50, 100, alpha=0.2, color='red', label='ç½‘ç»œæ‹¥å¡æœŸ')
    ax1.axvspan(100, 150, alpha=0.2, color='purple', label='é¢‘ç¹åˆ‡æ¢æœŸ')
    
    # æ ‡è®°LLMè°ƒç”¨ç‚¹
    for t, _ in llm_enhanced.llm_suggestions:
        if t < len(llm_enhanced.qos_history):
            ax1.axvline(x=t, color='red', linestyle='--', alpha=0.7, linewidth=1)
    
    ax1.set_title('ä¸»è¦QoSæ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax1.set_xlabel('æ—¶é—´æ§½')
    ax1.set_ylabel('å¹³å‡QoS')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. è¯­ä¹‰é€šä¿¡æŒ‡æ ‡å¯¹æ¯” (å³ä¸Šè§’)
    ax2 = fig.add_subplot(gs[0, 2])
    ax2.plot(time_steps, baseline.semantic_accuracy_history, 'b-', linewidth=2, label='å›ºå®šå‚æ•°')
    ax2.plot(time_steps, llm_enhanced.semantic_accuracy_history, 'darkorange', linewidth=2, label='åŠ¨æ€å‚æ•°')
    ax2.set_title('è¯­ä¹‰å‡†ç¡®ç‡', fontsize=12, fontweight='bold')
    ax2.set_ylabel('å‡†ç¡®ç‡')
    ax2.legend(fontsize=8)
    ax2.grid(True, alpha=0.3)
    
    ax3 = fig.add_subplot(gs[0, 3])
    ax3.plot(time_steps, baseline.semantic_rate_history, 'b-', linewidth=2, label='å›ºå®šå‚æ•°')
    ax3.plot(time_steps, llm_enhanced.semantic_rate_history, 'darkorange', linewidth=2, label='åŠ¨æ€å‚æ•°')
    ax3.set_title('è¯­ä¹‰é€Ÿç‡', fontsize=12, fontweight='bold')
    ax3.set_ylabel('é€Ÿç‡')
    ax3.legend(fontsize=8)
    ax3.grid(True, alpha=0.3)
    
    # 3. æ–°å¢è¯­ä¹‰é€šä¿¡ç‰¹æ€§å¯¹æ¯”
    ax4 = fig.add_subplot(gs[1, 2])
    ax4.plot(time_steps, baseline.compression_efficiency_history, 'b-', linewidth=2, label='å›ºå®šå‚æ•°')
    ax4.plot(time_steps, llm_enhanced.compression_efficiency_history, 'darkorange', linewidth=2, label='åŠ¨æ€å‚æ•°')
    ax4.set_title('è¯­ä¹‰å‹ç¼©æ•ˆç‡', fontsize=12, fontweight='bold')
    ax4.set_ylabel('å‹ç¼©æ•ˆç‡')
    ax4.legend(fontsize=8)
    ax4.grid(True, alpha=0.3)
    
    ax5 = fig.add_subplot(gs[1, 3])
    ax5.plot(time_steps, baseline.power_efficiency_history, 'b-', linewidth=2, label='å›ºå®šå‚æ•°')
    ax5.plot(time_steps, llm_enhanced.power_efficiency_history, 'darkorange', linewidth=2, label='åŠ¨æ€å‚æ•°')
    ax5.set_title('åŠŸç‡æ•ˆç‡', fontsize=12, fontweight='bold')
    ax5.set_ylabel('åŠŸç‡æ•ˆç‡')
    ax5.legend(fontsize=8)
    ax5.grid(True, alpha=0.3)
    
    # 4. 6å‚æ•°åŠ¨æ€è°ƒæ•´å†å² (ç¬¬ä¸‰è¡Œ)
    ax6 = fig.add_subplot(gs[2, :2])
    param_names = ['Alpha', 'Zeta', 'Omega']
    colors = ['red', 'green', 'blue']
    for i, (param, color) in enumerate(zip(['alpha', 'zeta', 'omega'], colors)):
        ax6.plot(time_steps, llm_enhanced.parameter_history[param], 
                color=color, linewidth=2, label=f'{param_names[i]}')
    ax6.set_title('LLMåŠ¨æ€å‚æ•°è°ƒæ•´ (åŸºç¡€å‚æ•°)', fontsize=12, fontweight='bold')
    ax6.set_xlabel('æ—¶é—´æ§½')
    ax6.set_ylabel('å‚æ•°å€¼')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    ax7 = fig.add_subplot(gs[2, 2:])
    param_names_ext = ['å‹ç¼©æ¯”', 'åŠŸç‡æ¯”', 'æœ€å°Ï†']
    colors_ext = ['purple', 'orange', 'brown']
    for param, color, name in zip(['compression_ratio', 'power_ratio', 'min_phi'], colors_ext, param_names_ext):
        ax7.plot(time_steps, llm_enhanced.parameter_history[param], 
                color=color, linewidth=2, label=name)
    ax7.set_title('LLMåŠ¨æ€å‚æ•°è°ƒæ•´ (SC-QOSå‚æ•°)', fontsize=12, fontweight='bold')
    ax7.set_xlabel('æ—¶é—´æ§½')
    ax7.set_ylabel('å‚æ•°å€¼')
    ax7.legend()
    ax7.grid(True, alpha=0.3)
    
    # 5. ç”¨æˆ·ä½“éªŒå’Œç³»ç»Ÿæ•ˆç‡ (ç¬¬å››è¡Œ)
    ax8 = fig.add_subplot(gs[3, 0])
    ax8.plot(time_steps, baseline.qoe_history, 'b-', linewidth=2, label='å›ºå®šå‚æ•°')
    ax8.plot(time_steps, llm_enhanced.qoe_history, 'darkorange', linewidth=2, label='åŠ¨æ€å‚æ•°')
    ax8.set_title('ç”¨æˆ·ä½“éªŒè´¨é‡(QoE)', fontsize=12, fontweight='bold')
    ax8.set_xlabel('æ—¶é—´æ§½')
    ax8.set_ylabel('QoEè¯„åˆ†')
    ax8.legend(fontsize=8)
    ax8.grid(True, alpha=0.3)
    
    ax9 = fig.add_subplot(gs[3, 1])
    ax9.plot(time_steps, baseline.resource_efficiency_history, 'b-', linewidth=2, label='å›ºå®šå‚æ•°')
    ax9.plot(time_steps, llm_enhanced.resource_efficiency_history, 'darkorange', linewidth=2, label='åŠ¨æ€å‚æ•°')
    ax9.set_title('èµ„æºæ•ˆç‡', fontsize=12, fontweight='bold')
    ax9.set_xlabel('æ—¶é—´æ§½')
    ax9.set_ylabel('æ•ˆç‡')
    ax9.legend(fontsize=8)
    ax9.grid(True, alpha=0.3)
    
    ax10 = fig.add_subplot(gs[3, 2])
    ax10.plot(time_steps, baseline.energy_consumption_history, 'b-', linewidth=2, label='å›ºå®šå‚æ•°')
    ax10.plot(time_steps, llm_enhanced.energy_consumption_history, 'darkorange', linewidth=2, label='åŠ¨æ€å‚æ•°')
    ax10.set_title('èƒ½è€—æŒ‡æ ‡', fontsize=12, fontweight='bold')
    ax10.set_xlabel('æ—¶é—´æ§½')
    ax10.set_ylabel('èƒ½è€—')
    ax10.legend(fontsize=8)
    ax10.grid(True, alpha=0.3)
    
    # 6. æ€§èƒ½æ”¹å–„ç‡
    improvement_rates = calculate_improvement_rate(baseline.qos_history, llm_enhanced.qos_history)
    ax11 = fig.add_subplot(gs[3, 3])
    positive_rates = [max(0, rate) for rate in improvement_rates]
    negative_rates = [min(0, rate) for rate in improvement_rates]
    ax11.fill_between(time_steps, 0, positive_rates, alpha=0.7, color='lightgreen', label='æ€§èƒ½æå‡')
    ax11.fill_between(time_steps, 0, negative_rates, alpha=0.7, color='lightcoral', label='æ€§èƒ½ä¸‹é™')
    ax11.plot(time_steps, improvement_rates, 'k-', linewidth=1.5, alpha=0.8)
    ax11.axhline(y=0, color='black', linestyle='-', alpha=0.5)
    ax11.set_title('æ€§èƒ½æ”¹å–„ç‡', fontsize=12, fontweight='bold')
    ax11.set_xlabel('æ—¶é—´æ§½')
    ax11.set_ylabel('æ”¹å–„ç‡ (%)')
    ax11.legend(fontsize=8)
    ax11.grid(True, alpha=0.3)
    
    plt.suptitle('å¢å¼ºç‰ˆMTUCB + Ollama LLM å…¨é¢æ€§èƒ½åˆ†æ', fontsize=16, fontweight='bold')
    plt.savefig('enhanced_mtucb_comprehensive_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()


def create_radar_chart_comparison(baseline, llm_enhanced):
    """åˆ›å»ºé›·è¾¾å›¾å¯¹æ¯”å¤šç»´åº¦æ€§èƒ½"""
    categories = [
        'QoS', 'è¯­ä¹‰å‡†ç¡®ç‡', 'è¯­ä¹‰é€Ÿç‡', 'å‹ç¼©æ•ˆç‡', 
        'åŠŸç‡æ•ˆç‡', 'QoE', 'èµ„æºæ•ˆç‡', 'ç³»ç»Ÿç¨³å®šæ€§'
    ]
    
    # è®¡ç®—å¹³å‡å€¼å¹¶å½’ä¸€åŒ–
    baseline_values = [
        np.mean(baseline.qos_history),
        np.mean(baseline.semantic_accuracy_history),
        np.mean(baseline.semantic_rate_history),
        np.mean(baseline.compression_efficiency_history),
        np.mean(baseline.power_efficiency_history),
        np.mean(baseline.qoe_history),
        np.mean(baseline.resource_efficiency_history) / 2,  # å½’ä¸€åŒ–
        1 - np.std(baseline.qos_history)  # ç¨³å®šæ€§
    ]
    
    llm_values = [
        np.mean(llm_enhanced.qos_history),
        np.mean(llm_enhanced.semantic_accuracy_history),
        np.mean(llm_enhanced.semantic_rate_history),
        np.mean(llm_enhanced.compression_efficiency_history),
        np.mean(llm_enhanced.power_efficiency_history),
        np.mean(llm_enhanced.qoe_history),
        np.mean(llm_enhanced.resource_efficiency_history) / 2,  # å½’ä¸€åŒ–
        1 - np.std(llm_enhanced.qos_history)  # ç¨³å®šæ€§
    ]
    
    # åˆ›å»ºé›·è¾¾å›¾
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆåœ†å½¢
    
    baseline_values += baseline_values[:1]
    llm_values += llm_values[:1]
    
    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
    
    ax.plot(angles, baseline_values, 'o-', linewidth=2, label='å›ºå®šå‚æ•°MTUCB', color='blue')
    ax.fill(angles, baseline_values, alpha=0.25, color='blue')
    
    ax.plot(angles, llm_values, 'o-', linewidth=2, label='åŠ¨æ€å‚æ•°MTUCB+Ollama', color='orange')
    ax.fill(angles, llm_values, alpha=0.25, color='orange')
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_ylim(0, 1)
    ax.set_title('å¤šç»´åº¦æ€§èƒ½é›·è¾¾å›¾å¯¹æ¯”', size=16, fontweight='bold', pad=20)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
    ax.grid(True)
    
    plt.savefig('enhanced_radar_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()


def create_heatmap_analysis(baseline, llm_enhanced):
    """åˆ›å»ºçƒ­åŠ›å›¾åˆ†æ"""
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. å‚æ•°å˜åŒ–çƒ­åŠ›å›¾
    params_data = np.array([
        llm_enhanced.parameter_history['alpha'],
        llm_enhanced.parameter_history['zeta'],
        llm_enhanced.parameter_history['omega'],
        llm_enhanced.parameter_history['compression_ratio'],
        llm_enhanced.parameter_history['power_ratio'],
        llm_enhanced.parameter_history['min_phi']
    ])
    
    param_labels = ['Alpha', 'Zeta', 'Omega', 'å‹ç¼©æ¯”', 'åŠŸç‡æ¯”', 'æœ€å°Ï†']
    
    # é‡é‡‡æ ·æ•°æ®ä»¥ä¾¿å¯è§†åŒ–
    sample_interval = max(1, len(llm_enhanced.parameter_history['alpha']) // 50)
    sampled_data = params_data[:, ::sample_interval]
    time_labels = list(range(0, len(llm_enhanced.parameter_history['alpha']), sample_interval))
    
    im1 = axes[0, 0].imshow(sampled_data, cmap='RdYlBu_r', aspect='auto')
    axes[0, 0].set_title('LLMå‚æ•°åŠ¨æ€è°ƒæ•´çƒ­åŠ›å›¾', fontweight='bold')
    axes[0, 0].set_xlabel('æ—¶é—´æ§½')
    axes[0, 0].set_ylabel('å‚æ•°')
    axes[0, 0].set_yticks(range(len(param_labels)))
    axes[0, 0].set_yticklabels(param_labels)
    plt.colorbar(im1, ax=axes[0, 0])
    
    # 2. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”çƒ­åŠ›å›¾
    metrics_baseline = np.array([
        baseline.qos_history[::sample_interval],
        baseline.semantic_accuracy_history[::sample_interval],
        baseline.compression_efficiency_history[::sample_interval],
        baseline.power_efficiency_history[::sample_interval],
        baseline.qoe_history[::sample_interval]
    ])
    
    metrics_llm = np.array([
        llm_enhanced.qos_history[::sample_interval],
        llm_enhanced.semantic_accuracy_history[::sample_interval],
        llm_enhanced.compression_efficiency_history[::sample_interval],
        llm_enhanced.power_efficiency_history[::sample_interval],
        llm_enhanced.qoe_history[::sample_interval]
    ])
    
    metric_labels = ['QoS', 'è¯­ä¹‰å‡†ç¡®ç‡', 'å‹ç¼©æ•ˆç‡', 'åŠŸç‡æ•ˆç‡', 'QoE']
    
    im2 = axes[0, 1].imshow(metrics_baseline, cmap='viridis', aspect='auto')
    axes[0, 1].set_title('å›ºå®šå‚æ•°æ€§èƒ½çƒ­åŠ›å›¾', fontweight='bold')
    axes[0, 1].set_xlabel('æ—¶é—´æ§½')
    axes[0, 1].set_ylabel('æ€§èƒ½æŒ‡æ ‡')
    axes[0, 1].set_yticks(range(len(metric_labels)))
    axes[0, 1].set_yticklabels(metric_labels)
    plt.colorbar(im2, ax=axes[0, 1])
    
    im3 = axes[1, 0].imshow(metrics_llm, cmap='viridis', aspect='auto')
    axes[1, 0].set_title('åŠ¨æ€å‚æ•°æ€§èƒ½çƒ­åŠ›å›¾', fontweight='bold')
    axes[1, 0].set_xlabel('æ—¶é—´æ§½')
    axes[1, 0].set_ylabel('æ€§èƒ½æŒ‡æ ‡')
    axes[1, 0].set_yticks(range(len(metric_labels)))
    axes[1, 0].set_yticklabels(metric_labels)
    plt.colorbar(im3, ax=axes[1, 0])
    
    # 3. æ”¹å–„çŸ©é˜µ
    improvement_matrix = metrics_llm - metrics_baseline
    im4 = axes[1, 1].imshow(improvement_matrix, cmap='RdBu_r', aspect='auto', vmin=-0.2, vmax=0.2)
    axes[1, 1].set_title('æ€§èƒ½æ”¹å–„çƒ­åŠ›å›¾', fontweight='bold')
    axes[1, 1].set_xlabel('æ—¶é—´æ§½')
    axes[1, 1].set_ylabel('æ€§èƒ½æŒ‡æ ‡')
    axes[1, 1].set_yticks(range(len(metric_labels)))
    axes[1, 1].set_yticklabels(metric_labels)
    plt.colorbar(im4, ax=axes[1, 1])
    
    plt.tight_layout()
    plt.savefig('enhanced_heatmap_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()


def create_optimization_effectiveness_plot(llm_enhanced):
    """åˆ›å»ºä¼˜åŒ–æ•ˆæœåˆ†æå›¾"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. LLMè°ƒç”¨æ—¶é—´å’Œç½®ä¿¡åº¦
    call_times = [t for t, _ in llm_enhanced.llm_suggestions]
    confidences = [s.confidence for _, s in llm_enhanced.llm_suggestions]
    
    axes[0, 0].scatter(call_times, confidences, c=confidences, cmap='RdYlGn', s=100)
    axes[0, 0].set_title('LLMå»ºè®®ç½®ä¿¡åº¦å˜åŒ–', fontweight='bold')
    axes[0, 0].set_xlabel('æ—¶é—´æ§½')
    axes[0, 0].set_ylabel('ç½®ä¿¡åº¦')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. å‚æ•°å˜åŒ–å¹…åº¦
    param_changes = []
    for i in range(1, len(llm_enhanced.parameter_history['alpha'])):
        change = abs(llm_enhanced.parameter_history['alpha'][i] - llm_enhanced.parameter_history['alpha'][i-1])
        change += abs(llm_enhanced.parameter_history['zeta'][i] - llm_enhanced.parameter_history['zeta'][i-1])
        change += abs(llm_enhanced.parameter_history['omega'][i] - llm_enhanced.parameter_history['omega'][i-1])
        param_changes.append(change)
    
    axes[0, 1].plot(range(1, len(param_changes)+1), param_changes, 'r-', linewidth=2)
    axes[0, 1].set_title('å‚æ•°å˜åŒ–å¹…åº¦', fontweight='bold')
    axes[0, 1].set_xlabel('æ—¶é—´æ§½')
    axes[0, 1].set_ylabel('æ€»å˜åŒ–å¹…åº¦')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æ€§èƒ½æ”¹å–„è¶‹åŠ¿
    window_size = 20
    performance_trend = []
    for i in range(window_size, len(llm_enhanced.qos_history)):
        recent_avg = np.mean(llm_enhanced.qos_history[i-window_size:i])
        performance_trend.append(recent_avg)
    
    axes[1, 0].plot(range(window_size, len(llm_enhanced.qos_history)), performance_trend, 
                   'g-', linewidth=2, label='æ»‘åŠ¨å¹³å‡QoS')
    axes[1, 0].set_title('æ€§èƒ½æ”¹å–„è¶‹åŠ¿', fontweight='bold')
    axes[1, 0].set_xlabel('æ—¶é—´æ§½')
    axes[1, 0].set_ylabel('QoS')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. å„æŒ‡æ ‡ç›¸å…³æ€§åˆ†æ
    if len(llm_enhanced.metrics_history) > 50:
        recent_metrics = llm_enhanced.metrics_history[-50:]
        correlation_data = np.array([
            [m.avg_qos for m in recent_metrics],
            [m.semantic_accuracy for m in recent_metrics],
            [m.semantic_compression_efficiency for m in recent_metrics],
            [m.power_efficiency for m in recent_metrics],
            [m.qoe_score for m in recent_metrics]
        ])
        
        correlation_matrix = np.corrcoef(correlation_data)
        labels = ['QoS', 'è¯­ä¹‰å‡†ç¡®ç‡', 'å‹ç¼©æ•ˆç‡', 'åŠŸç‡æ•ˆç‡', 'QoE']
        
        im = axes[1, 1].imshow(correlation_matrix, cmap='coolwarm', vmin=-1, vmax=1)
        axes[1, 1].set_title('æ€§èƒ½æŒ‡æ ‡ç›¸å…³æ€§', fontweight='bold')
        axes[1, 1].set_xticks(range(len(labels)))
        axes[1, 1].set_yticks(range(len(labels)))
        axes[1, 1].set_xticklabels(labels, rotation=45)
        axes[1, 1].set_yticklabels(labels)
        
        # æ·»åŠ æ•°å€¼æ ‡æ³¨
        for i in range(len(labels)):
            for j in range(len(labels)):
                axes[1, 1].text(j, i, f'{correlation_matrix[i, j]:.2f}',
                               ha="center", va="center", color="black")
        
        plt.colorbar(im, ax=axes[1, 1])
    
    plt.tight_layout()
    plt.savefig('optimization_effectiveness_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()


def calculate_improvement_rate(baseline_values, llm_values, window_size=10):
    """è®¡ç®—æ»‘åŠ¨çª—å£æ”¹å–„ç‡"""
    improvement_rates = []
    
    for i in range(len(baseline_values)):
        if i < window_size:
            baseline_avg = np.mean(baseline_values[:i+1])
            llm_avg = np.mean(llm_values[:i+1])
        else:
            baseline_avg = np.mean(baseline_values[i-window_size:i+1])
            llm_avg = np.mean(llm_values[i-window_size:i+1])
        
        if baseline_avg > 0:
            improvement = ((llm_avg - baseline_avg) / baseline_avg) * 100
        else:
            improvement = 0
        
        improvement_rates.append(improvement)
    
    return improvement_rates


def print_enhanced_comparison_statistics(baseline, llm_enhanced):
    """æ‰“å°å¢å¼ºçš„å¯¹æ¯”ç»Ÿè®¡ç»“æœ"""
    print("\n" + "="*80)
    print("ğŸ“Š å¢å¼ºç‰ˆMTUCB vs MTUCB+Ollama å…¨é¢æ€§èƒ½åˆ†æ")
    print("="*80)
    
    # åŸºç¡€æ€§èƒ½å¯¹æ¯”
    baseline_qos = np.mean(baseline.qos_history)
    llm_qos = np.mean(llm_enhanced.qos_history)
    qos_improvement = ((llm_qos - baseline_qos) / baseline_qos) * 100
    
    print(f"ğŸ¯ åŸºç¡€æ€§èƒ½å¯¹æ¯”:")
    print(f"   å¹³å‡QoS: {baseline_qos:.4f} â†’ {llm_qos:.4f} ({qos_improvement:+.2f}%)")
    
    # è¯­ä¹‰é€šä¿¡æŒ‡æ ‡å¯¹æ¯”
    metrics_comparison = {
        'è¯­ä¹‰å‡†ç¡®ç‡': (baseline.semantic_accuracy_history, llm_enhanced.semantic_accuracy_history),
        'è¯­ä¹‰é€Ÿç‡': (baseline.semantic_rate_history, llm_enhanced.semantic_rate_history),
        'å‹ç¼©æ•ˆç‡': (baseline.compression_efficiency_history, llm_enhanced.compression_efficiency_history),
        'åŠŸç‡æ•ˆç‡': (baseline.power_efficiency_history, llm_enhanced.power_efficiency_history),
        'ç”¨æˆ·ä½“éªŒ(QoE)': (baseline.qoe_history, llm_enhanced.qoe_history),
        'èµ„æºæ•ˆç‡': (baseline.resource_efficiency_history, llm_enhanced.resource_efficiency_history),
        'èƒ½è€—æŒ‡æ ‡': (baseline.energy_consumption_history, llm_enhanced.energy_consumption_history),
    }
    
    print(f"\nğŸ”¬ è¯­ä¹‰é€šä¿¡ä¸ç³»ç»Ÿæ•ˆç‡å¯¹æ¯”:")
    for metric_name, (baseline_values, llm_values) in metrics_comparison.items():
        baseline_avg = np.mean(baseline_values)
        llm_avg = np.mean(llm_values)
        improvement = ((llm_avg - baseline_avg) / baseline_avg) * 100
        print(f"   {metric_name}: {baseline_avg:.4f} â†’ {llm_avg:.4f} ({improvement:+.2f}%)")
    
    # ç½‘ç»œé€‚åº”æ€§åˆ†æ
    print(f"\nğŸŒ ç½‘ç»œçŠ¶æ€é€‚åº”æ€§:")
    
    # æ‹¥å¡æœŸè¡¨ç°
    congestion_period_baseline = np.mean(baseline.qos_history[50:101])
    congestion_period_llm = np.mean(llm_enhanced.qos_history[50:101])
    congestion_improvement = ((congestion_period_llm - congestion_period_baseline) / congestion_period_baseline) * 100
    
    # é¢‘ç¹åˆ‡æ¢æœŸè¡¨ç°
    switch_period_baseline = np.mean(baseline.qos_history[100:151])
    switch_period_llm = np.mean(llm_enhanced.qos_history[100:151])
    switch_improvement = ((switch_period_llm - switch_period_baseline) / switch_period_baseline) * 100
    
    print(f"   æ‹¥å¡æœŸQoSæ”¹å–„: {congestion_improvement:+.2f}%")
    print(f"   é¢‘ç¹åˆ‡æ¢æœŸQoSæ”¹å–„: {switch_improvement:+.2f}%")
    
    # LLMä¼˜åŒ–æ•ˆæœ
    if llm_enhanced.llm_call_times:
        print(f"\nğŸ¤– LLMä¼˜åŒ–æ•ˆæœ:")
        print(f"   æ€»è°ƒç”¨æ¬¡æ•°: {len(llm_enhanced.llm_call_times)}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {np.mean(llm_enhanced.llm_call_times):.1f}ç§’")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {np.mean(llm_enhanced.confidence_history):.3f}")
        
        # å‚æ•°å˜åŒ–ç»Ÿè®¡
        param_changes = {
            'alpha': np.std(llm_enhanced.parameter_history['alpha']),
            'zeta': np.std(llm_enhanced.parameter_history['zeta']),
            'omega': np.std(llm_enhanced.parameter_history['omega']),
            'compression_ratio': np.std(llm_enhanced.parameter_history['compression_ratio']),
            'power_ratio': np.std(llm_enhanced.parameter_history['power_ratio']),
            'min_phi': np.std(llm_enhanced.parameter_history['min_phi'])
        }
        
        print(f"\nğŸ“Š å‚æ•°åŠ¨æ€è°ƒæ•´èŒƒå›´:")
        for param, std_val in param_changes.items():
            print(f"   {param}: æ ‡å‡†å·® {std_val:.4f}")
    
    # ç³»ç»Ÿç¨³å®šæ€§åˆ†æ
    baseline_stability = 1 - np.std(baseline.qos_history)
    llm_stability = 1 - np.std(llm_enhanced.qos_history)
    
    print(f"\nâš–ï¸ ç³»ç»Ÿç¨³å®šæ€§:")
    print(f"   å›ºå®šå‚æ•°ç¨³å®šæ€§: {baseline_stability:.4f}")
    print(f"   åŠ¨æ€å‚æ•°ç¨³å®šæ€§: {llm_stability:.4f}")
    print(f"   ç¨³å®šæ€§å˜åŒ–: {(llm_stability - baseline_stability):+.4f}")
    
    print("="*80)


def main():
    """ä¸»å‡½æ•° - è¿è¡Œå¢å¼ºç‰ˆå¯¹æ¯”å®éªŒ"""
    print("ğŸ¯ å¢å¼ºç‰ˆMTUCB + Ollama LLM å…¨é¢å¯¹æ¯”å®éªŒ")
    print("="*60)
    
    # å®éªŒå‚æ•°
    num_users = 12
    num_workers = 6
    num_paths = 4
    T = 200
    llm_period = 25
    
    print(f"ğŸ“‹ å®éªŒé…ç½®:")
    print(f"   ç”¨æˆ·æ•°: {num_users}, å·¥äººæ•°: {num_workers}, è·¯å¾„æ•°: {num_paths}")
    print(f"   æ—¶é—´æ§½: {T}, LLMè°ƒç”¨å‘¨æœŸ: {llm_period}")
    print(f"   ä¼˜åŒ–å‚æ•°: 6ä¸ª (alpha, zeta, omega, compression_ratio, power_ratio, min_phi)")
    
    # è¿è¡ŒåŸºç¡€ç®—æ³•
    print("\nğŸ”µ è¿è¡Œå¢å¼ºç‰ˆåŸºç¡€MTUCBç®—æ³•ï¼ˆå›ºå®šå‚æ•°ï¼‰...")
    baseline = EnhancedMTUCBBaseline(num_users, num_workers, num_paths)
    baseline.run_simulation(T)
    print("âœ… åŸºç¡€ç®—æ³•å®Œæˆ")
    
    # è¿è¡ŒLLMå¢å¼ºç®—æ³•
    print("\nğŸŸ  è¿è¡Œå¢å¼ºç‰ˆMTUCB + Ollamaç®—æ³•ï¼ˆ6å‚æ•°åŠ¨æ€ä¼˜åŒ–ï¼‰...")
    llm_enhanced = EnhancedMTUCBWithOllama(
        num_users, num_workers, num_paths,
        llm_model="tinyllama", llm_period=llm_period
    )
    llm_enhanced.run_simulation(T)
    print("âœ… LLMå¢å¼ºç®—æ³•å®Œæˆ")
    
    # ç”Ÿæˆå…¨é¢çš„å¯è§†åŒ–åˆ†æ
    print("\nğŸ“Š ç”Ÿæˆå…¨é¢æ€§èƒ½åˆ†æå›¾è¡¨...")
    
    print("   ğŸ“ˆ åˆ›å»ºç»¼åˆå¯¹æ¯”å›¾...")
    create_comprehensive_comparison_plots(baseline, llm_enhanced)
    
    print("   ğŸ¯ åˆ›å»ºé›·è¾¾å›¾å¯¹æ¯”...")
    create_radar_chart_comparison(baseline, llm_enhanced)
    
    print("   ğŸ”¥ åˆ›å»ºçƒ­åŠ›å›¾åˆ†æ...")
    create_heatmap_analysis(baseline, llm_enhanced)
    
    print("   âš¡ åˆ›å»ºä¼˜åŒ–æ•ˆæœåˆ†æ...")
    create_optimization_effectiveness_plot(llm_enhanced)
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡ç»“æœ
    print_enhanced_comparison_statistics(baseline, llm_enhanced)
    
    print("\nğŸ‰ å¢å¼ºç‰ˆå®éªŒå®Œæˆï¼")
    print("ğŸ’¡ ç”Ÿæˆçš„å›¾è¡¨æ–‡ä»¶:")
    print("   â€¢ enhanced_mtucb_comprehensive_analysis.png - ç»¼åˆæ€§èƒ½åˆ†æ")
    print("   â€¢ enhanced_radar_comparison.png - å¤šç»´åº¦é›·è¾¾å›¾å¯¹æ¯”") 
    print("   â€¢ enhanced_heatmap_analysis.png - çƒ­åŠ›å›¾åˆ†æ")
    print("   â€¢ optimization_effectiveness_analysis.png - ä¼˜åŒ–æ•ˆæœåˆ†æ")
    
    # ä¿å­˜æ•°æ®ç”¨äºè¿›ä¸€æ­¥åˆ†æ
    results_summary = {
        'baseline_avg_qos': np.mean(baseline.qos_history),
        'llm_avg_qos': np.mean(llm_enhanced.qos_history),
        'qos_improvement': ((np.mean(llm_enhanced.qos_history) - np.mean(baseline.qos_history)) / np.mean(baseline.qos_history)) * 100,
        'llm_calls': len(llm_enhanced.llm_suggestions),
        'avg_confidence': np.mean(llm_enhanced.confidence_history) if llm_enhanced.confidence_history else 0,
        'avg_response_time': np.mean(llm_enhanced.llm_call_times) if llm_enhanced.llm_call_times else 0
    }
    
    print(f"\nğŸ“‹ å®éªŒæ€»ç»“:")
    print(f"   æ€»ä½“QoSæ”¹å–„: {results_summary['qos_improvement']:+.2f}%")
    print(f"   LLMè°ƒç”¨æ¬¡æ•°: {results_summary['llm_calls']}")
    print(f"   å¹³å‡ç½®ä¿¡åº¦: {results_summary['avg_confidence']:.3f}")
    print(f"   å¹³å‡å“åº”æ—¶é—´: {results_summary['avg_response_time']:.1f}ç§’")


if __name__ == "__main__":
    main() 
